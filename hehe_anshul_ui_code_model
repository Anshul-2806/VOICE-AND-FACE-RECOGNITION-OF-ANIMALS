pip install numpy pyaudio tensorflow librosa scikit-learn
Step 1: Feature Extraction First, you'll need to extract features from your audio files. This example assumes you have a dataset of .wav files organized in two folders: dog/ and cat/.

import numpy as np
import librosa
import os

def extract_features(file_path):
    # Load the audio file
    signal, sr = librosa.load(file_path, sr=None)
    
    # Extract MFCCs
    mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13)
    
    # Take the mean of MFCCs across time
    mfccs_mean = np.mean(mfccs.T, axis=0)
    return mfccs_mean

data_dir = 'path_to_your_dataset'  # Update this to your dataset path
features = []
labels = []

for label in ['dog', 'cat']:
    for filename in os.listdir(os.path.join(data_dir, label)):
        if filename.endswith('.wav'):
            file_path = os.path.join(data_dir, label, filename)
            mfccs = extract_features(file_path)
            features.append(mfccs)
            labels.append(label)

# Convert to NumPy arrays
X = np.array(features)
y = np.array(labels)

Step 2: Model Training

from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import LabelEncoder

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert labels to numerical values
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)

# Create a simple neural network
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))
Step 3: Making Predictions
Now that the model is trained, you can predict the class of new audio files.

python
Copy code
def predict_sound(file_path):
    features = extract_features(file_path).reshape(1, -1)  # Reshape for model input
    prediction = model.predict(features)
    predicted_label = le.inverse_transform([np.argmax(prediction)])
    return predicted_label[0]

# Example usage
result = predict_sound('path_to_new_sound.wav')
print("The sound is from a:", result)  # Output: The sound is from a: dog or cat

Step 4: If you want to implement real-time recognition, you'll need to capture audio using pyaudio and process it in a similar manner. Here’s a very basic outline:

import pyaudio
import wave

def record_audio(filename):
    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 44100
    RECORD_SECONDS = 5

    audio = pyaudio.PyAudio()

    stream = audio.open(format=FORMAT, channels=CHANNELS,
                        rate=RATE, input=True,
                        frames_per_buffer=CHUNK)

    print("Recording...")
    frames = []

    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
        data = stream.read(CHUNK)
        frames.append(data)

    print("Finished recording.")

    stream.stop_stream()
    stream.close()
    audio.terminate()

    with wave.open(filename, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(audio.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))

# Record a new audio sample
record_audio('new_sound.wav')

# Predict the recorded sound
result = predict_sound('new_sound.wav')
print("The recorded sound is from a:", result)

=======================================================================================================
To apply an Animal Voice and Face Recognition model in a real-world setting, we can develop a web application where users upload images or audio samples of animals to get recognition results. This application would use machine learning models on the backend to process voice and face data and return predictions to the user via the frontend.

1. Setting Up the Backend (Flask + Model Integration)
We’ll use Flask for the backend, TensorFlow/Keras to load pre-trained models, and basic data processing tools to handle images and audio.

a) Install Required Libraries
pip install Flask tensorflow librosa numpy pillow

b) Define the Flask API for Voice and Face Recognition
from flask import Flask, request, jsonify
import numpy as np
import tensorflow as tf
from PIL import Image
import librosa

app = Flask(__name__)

# Load pre-trained models
face_model = tf.keras.models.load_model("models/face_model.h5")
voice_model = tf.keras.models.load_model("models/voice_model.h5")

def preprocess_image(image_path):
    image = Image.open(image_path)
    image = image.resize((224, 224))
    image_array = np.array(image) / 255.0  # Normalize
    return np.expand_dims(image_array, axis=0)

def preprocess_audio(audio_path):
    y, sr = librosa.load(audio_path, sr=22050)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    mfcc = np.mean(mfcc.T, axis=0)
    return np.expand_dims(mfcc, axis=0)

@app.route('/predict_face', methods=['POST'])
def predict_face():
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400
    file = request.files['file']
    image_path = 'uploads/' + file.filename
    file.save(image_path)

    image_array = preprocess_image(image_path)
    prediction = face_model.predict(image_array)
    animal_class = np.argmax(prediction)

    return jsonify({'prediction': int(animal_class)})

@app.route('/predict_voice', methods=['POST'])
def predict_voice():
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400
    file = request.files['file']
    audio_path = 'uploads/' + file.filename
    file.save(audio_path)

    audio_features = preprocess_audio(audio_path)
    prediction = voice_model.predict(audio_features)
    animal_class = np.argmax(prediction)

    return jsonify({'prediction': int(animal_class)})

if __name__ == '__main__':
    app.run(debug=True)
Explanation:
Image Preprocessing: Resizes and normalizes images to feed into the face recognition model.
Audio Preprocessing: Extracts MFCC (Mel Frequency Cepstral Coefficients) features, which are suitable for audio classification.
Endpoints: /predict_face and /predict_voice handle image and audio uploads, process the files, and return the animal prediction.

2. Frontend (React) for UI : allow users to upload images or audio files and view the recognition results.

a) Install Axios for API Requests
In your React project directory, install Axios:
npm install axios

b) Frontend Code : Below is a simple React component for uploading files to the Flask backend.

import React, { useState } from 'react';
import axios from 'axios';

const AnimalRecognition = () => {
  const [file, setFile] = useState(null);
  const [result, setResult] = useState("");

  const handleFileChange = (e) => {
    setFile(e.target.files[0]);
  };

  const handlePrediction = async (type) => {
    if (!file) {
      alert("Please upload a file.");
      return;
    }

    const formData = new FormData();
    formData.append("file", file);

    const endpoint = type === "face" ? "/predict_face" : "/predict_voice";
    try {
      const response = await axios.post(`http://localhost:5000${endpoint}`, formData);
      setResult(`Predicted animal class: ${response.data.prediction}`);
    } catch (error) {
      setResult("Error in prediction");
    }
  };

  return (
    <div>
      <h2>Animal Voice and Face Recognition</h2>
      <input type="file" onChange={handleFileChange} />
      <button onClick={() => handlePrediction("face")}>Predict Face</button>
      <button onClick={() => handlePrediction("voice")}>Predict Voice</button>
      <p>{result}</p>
    </div>
  );
};

export default AnimalRecognition;

Explanation : File Upload: The file input accepts images or audio files. The file is sent to the backend for processing.
Prediction Buttons: Two buttons call different endpoints, one for face recognition and one for voice recognition.
Display Result: The result from the backend is displayed to the user.

3. Running the Application Start the Flask Backend: Run the Flask application to serve your model predictions.

python app.py
Start the React Frontend: In your React project, start the frontend : npm start
Test the Application : Open the React app in a browser (default: http://localhost:3000).
Upload an image or audio file and click the appropriate button to get predictions.
The results will display on the page.

4. Deployment Options
Backend: Deploy the Flask app to a service like AWS.
Frontend: Deploy the React app on platforms like Vercel.
